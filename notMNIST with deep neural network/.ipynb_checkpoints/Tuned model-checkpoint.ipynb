{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#constructing/tuning a better model to increase the accurcy\n",
    "\"\"\"Techniques used in this model\n",
    "1. Stochastic gradient method instead of gradient method for loss optimization\n",
    "2. Relu as activation funtion for hidden layer (others such as tanh mightbe tried later)\n",
    "3. L2 for regularization\n",
    "4. Dropout with regularization\n",
    "5. Multiple hidden layers\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#local tunables\n",
    "batch_size = 256\n",
    "hidden_units1 = 1024\n",
    "hidden_units2 = 1024\n",
    "\n",
    "beta = 0.01\n",
    "\n",
    "tuned_graph = tf.Graph()\n",
    "with tuned_graph.as_default():\n",
    "\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "  weights1 = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_units1]))\n",
    "  biases1 = tf.Variable(tf.zeros([hidden_units1]))\n",
    "  \n",
    "  hidden_output1 = tf.matmul(tf_train_dataset, weights1) + biases1\n",
    "  relu_output1 = tf.nn.relu(hidden_output1)\n",
    "    \n",
    "  keep_prob = tf.placeholder(\"float\")\n",
    "  relu_output1_drop = tf.nn.dropout(relu_output1, keep_prob)\n",
    "\n",
    "\n",
    "  weights2 = tf.Variable(tf.truncated_normal([hidden_units1,hidden_units2]))\n",
    "  biases2 = tf.Variable(tf.zeros([hidden_units2]))\n",
    "    \n",
    "  hidden_output2 = tf.matmul(relu_output1_drop, weights2) + biases2\n",
    "  relu_output2 = tf.nn.relu(hidden_output2)\n",
    "    \n",
    "  relu_output2_drop = tf.nn.dropout(relu_output2, keep_prob)\n",
    "    \n",
    "  weights3 = tf.Variable(tf.truncated_normal([hidden_units2, num_labels]))\n",
    "  biases3 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "  final_output = tf.matmul(relu_output2_drop, weights3) + biases3\n",
    "\n",
    "  #relu_final_output = tf.nn.relu(final_output)\n",
    "  #dropout_layer3 = tf.nn.relu(final_output_dropout)\n",
    "  \n",
    "    \n",
    "  loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(final_output, tf_train_labels))\n",
    "  loss = tf.reduce_mean(loss + beta * ( tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2) + tf.nn.l2_loss(weights3)))\n",
    "    \n",
    "  # Optimizer\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate_var = tf.placeholder(\"float\")\n",
    "  learning_rate = tf.train.exponential_decay(learning_rate_var, global_step, 100000, 0.95, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step= global_step)\n",
    "\n",
    "  relu_output1_valid = tf.nn.relu(tf.matmul(tf_valid_dataset,weights1) + biases1)    \n",
    "  relu_output2_valid = tf.nn.relu(tf.matmul(relu_output1_valid,weights2) + biases2)  \n",
    "\n",
    "  relu_output1_test = tf.nn.relu(tf.matmul(tf_test_dataset, weights1) + biases1)\n",
    "  relu_output2_test = tf.nn.relu(tf.matmul(relu_output1_test, weights2) + biases2)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(final_output)\n",
    "  valid_prediction = tf.nn.softmax(tf.matmul(relu_output2_valid, weights3) + biases3)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(relu_output2_test, weights3) + biases3)\n",
    "\n",
    "print(\"Graph with hidden layer created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_steps = 6001\n",
    "\n",
    "prob = 0.8\n",
    "lr = 0.001\n",
    "\n",
    "with tf.Session(graph=tuned_graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, keep_prob : prob, learning_rate_var : lr}\n",
    "    _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    \n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
